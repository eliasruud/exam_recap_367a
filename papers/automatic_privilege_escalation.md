# Automatic Privilege Escalation with Deep Reinforcement Learning

**DISCLAIMER**: Summarized by AI

## Problem they are trying to solve / Purpose of method

**What are the previous problems that need to be solved?**

- Existing attack automation tools (e.g., Metasploit, Cobalt Strike) are script-based and 
require human configuration tailored to specific environments. These generate predictable patterns, making them easier to detect.
- Machine learning-based defensive systems require large, realistic datasets of attacks for training, but collecting such data is a challenge.
- Current research has focused on automating simple or narrowly defined cyberattack tasks.
Automating complex attack stages, like **local privilege escalation**,
remains a difficult challenge due to long action sequences, partial observability, and vast action spaces.

**Why is the method introduced/needed?**

- To demonstrate that **deep reinforcement learning (DRL)** can be used to
automate **local privilege escalation**, a critical post-exploitation step.
- To provide a **realistic, intelligent red teaming agent** capable of learning from environment interaction,
and to generate high-fidelity training data for defensive systems.
- To explore the potential for malicious use of DRL in cybersecurity, thus helping defenders anticipate future threats.

## How does it differ from other methods?

**Compare what is different from other methods:**

- Prior work (e.g., DeepExploit) focused on **initial access** and treated **lateral movement** simplistically.
- Some DRL agents (like Maeda & Mimuraâ€™s) addressed **lateral movement**, but with limited state/action complexity.

**What makes this method unique?**

- First DRL agent to tackle **local privilege escalation** in a realistic Windows 7 environment.
- Uses a **rich, high-dimensional state space** (thousands of variables) and a **high-level action space** (38 atomic, manually designed actions).
- The environment is partially observable, with a large variety of possible system configurations and vulnerabilities (12 types).
- Training is done in a **custom simulator**, but the model transfers successfully to **real-world VMs** without retraining.
- Agent generalizes to **arbitrary numbers of services, DLLs, and tasks**, using neural network modules with shared parameters.

## How the method works

**Simple overview:**

- The authors use a **Deep Reinforcement Learning (A2C - Advantage Actor-Critic)** agent to learn how to perform **local privilege escalation** in a Windows environment.
- The agent interacts with a simulated environment that models Windows 7, including services, tasks, and DLLs, with randomized vulnerabilities in each episode.
- Once trained, the agent is evaluated in a **real VM** to verify generalization and effectiveness.

**More details:**

- **Reward Function:** Sparse reward; agent gets +1 if privilege escalation is successful, 0 otherwise.
- **Action Space:** 38 high-level actions (e.g., analyze DLLs, reconfigure service, overwrite binaries).
Actions are atomic and independent of low-level implementation.
- **State Representation:** Encodes info about services, DLLs, scheduled tasks, autoruns, credentials,
and system state using binary/trinary attributes.
- **Neural Network Architecture:** Modular processing for varying numbers of services and components;
uses max-pooling to combine component states.
- **Training:** Performed on a fast Python simulator (to avoid VM slowness), using A2C with shared weights for value and policy functions.
- **Testing:** Deployed in real Windows 7 VMs using SSH, successfully escalated privileges across all tested vulnerabilities.
Outperformed random and stochastic policies, closely matched expert rule-based policies.
